{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "882gPGOGM7-i"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqUHYdvRJ7pt"
      },
      "outputs": [],
      "source": [
        "#@title Install Tau-bench and GEPA\n",
        "!git clone https://github.com/sierra-research/tau-bench.git\n",
        "%cd tau-bench/\n",
        "!pip install -e . --quiet\n",
        "\n",
        "%cd ..\n",
        "!pip install gepa --quiet\n",
        "\n",
        "!pip install retry --quiet\n",
        "\n",
        "%cd tau-bench/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdGCJfEtz8Nq"
      },
      "outputs": [],
      "source": [
        "#@title Setup and Authentication\n",
        "import logging\n",
        "import os\n",
        "\n",
        "from google.genai import types\n",
        "#@markdown Configure Vertex AI Access\n",
        "\n",
        "GCP_PROJECT = '' #@param {type: 'string'}\n",
        "GCP_LOCATION = 'us-central1' #@param {type: 'string'}\n",
        "\n",
        "os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'true'\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = GCP_PROJECT\n",
        "os.environ['GOOGLE_CLOUD_LOCATION'] = GCP_LOCATION\n",
        "\n",
        "# Set a logging verbosity suited for this experiment. See\n",
        "# https://github.com/google/adk-python/issues/1852 for context\n",
        "class _FilterInferenceWarnings(logging.Filter):\n",
        "  \"\"\"Filters out Vertex inference warning about non-text parts in response.\"\"\"\n",
        "\n",
        "  def filter(self, record):\n",
        "    if record.levelname != 'WARNING':\n",
        "      return True\n",
        "    message_identifier = record.getMessage()\n",
        "    return not message_identifier.startswith(\n",
        "        'Warning: there are non-text parts in the response:'\n",
        "    )\n",
        "\n",
        "\n",
        "types.logger.addFilter(_FilterInferenceWarnings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gAERCrlhgiam"
      },
      "outputs": [],
      "source": [
        "#@title Discover Tau-bench Tool Definitions\n",
        "# This section dynamically discovers and imports tool definitions from the\n",
        "# tau-bench library. The code below is searches for the base 'Tool' class and\n",
        "# then looks for all subclasses in the 'retail' and 'airline' domains.\n",
        "import sys, pathlib, importlib, inspect, ast\n",
        "\n",
        "REPO_ROOT = pathlib.Path.cwd()\n",
        "PKG_ROOT = REPO_ROOT / 'tau_bench'\n",
        "DOMAINS = ['retail', 'airline']\n",
        "\n",
        "if str(REPO_ROOT) not in sys.path:\n",
        "  sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "def import_Tool():\n",
        "  \"\"\"Finds and imports the base `Tool` class from the tau-bench library.\n",
        "\n",
        "  It tries several known locations for the `Tool` class. As a fallback, it\n",
        "  searches for any file defining 'class Tool(' and attempts to import from\n",
        "  there.\n",
        "\n",
        "  Returns:\n",
        "    The `Tool` class object.\n",
        "\n",
        "  Raises:\n",
        "    ImportError: If the `Tool` class cannot be located.\n",
        "  \"\"\"\n",
        "  candidates = [\n",
        "    'tau_bench.core.tool:Tool',\n",
        "    'tau_bench.core.tools:Tool',\n",
        "    'tau_bench.tools:Tool',\n",
        "    'tau_bench.envs.core.tool:Tool',\n",
        "    'tau_bench.envs.core.tools:Tool',\n",
        "    'tau_bench.envs.tool:Tool',\n",
        "  ]\n",
        "  for spec in candidates:\n",
        "    mod, name = spec.split(':')\n",
        "    try:\n",
        "      return getattr(importlib.import_module(mod), name)\n",
        "    except Exception:\n",
        "      pass\n",
        "  # Fallback: find any file that defines 'class Tool(' and import it\n",
        "  for p in PKG_ROOT.rglob('*.py'):\n",
        "    try:\n",
        "      txt = p.read_text(encoding='utf-8', errors='ignore')\n",
        "    except Exception:\n",
        "      continue\n",
        "    if 'class Tool(' in txt:\n",
        "      mod = '.'.join(p.relative_to(REPO_ROOT).with_suffix('').parts)\n",
        "      try:\n",
        "        return getattr(importlib.import_module(mod), 'Tool')\n",
        "      except Exception:\n",
        "        pass\n",
        "  raise ImportError('Could not locate the Tool class in tau_bench')\n",
        "\n",
        "Tool = import_Tool()\n",
        "\n",
        "def module_name_from_path(pyfile: pathlib.Path) -> str:\n",
        "  rel = pyfile.relative_to(REPO_ROOT).with_suffix('')\n",
        "  return '.'.join(rel.parts)\n",
        "\n",
        "def ast_inherits_tool(pyfile: pathlib.Path) -> list:\n",
        "  \"\"\"Returns a list of fully-qualified class names.\n",
        "\n",
        "  These include 'Tool' or '*.Tool'.\n",
        "  \"\"\"\n",
        "  out = []\n",
        "  try:\n",
        "    code = pyfile.read_text(encoding='utf-8', errors='ignore')\n",
        "    tree = ast.parse(code, filename=str(pyfile))\n",
        "  except Exception:\n",
        "    return out\n",
        "  modname = module_name_from_path(pyfile)\n",
        "  for node in ast.walk(tree):\n",
        "    if isinstance(node, ast.ClassDef):\n",
        "      inherits = False\n",
        "      for base in node.bases:\n",
        "        try:\n",
        "          base_txt = ast.unparse(base)\n",
        "        except Exception:\n",
        "          if hasattr(base, \"id\"):\n",
        "            base_txt = base.id\n",
        "          elif hasattr(base, \"attr\"):\n",
        "            base_txt = base.attr\n",
        "          else:\n",
        "            base_txt = ''\n",
        "        if base_txt == 'Tool' or base_txt.endswith('.Tool'):\n",
        "          inherits = True\n",
        "          break\n",
        "      if inherits:\n",
        "        out.append(f'{modname}.{node.name}')\n",
        "  return out\n",
        "\n",
        "def collect_for_domain(domain: str) -> list:\n",
        "  \"\"\"Collects all tool classes for a given domain (e.g., 'retail').\n",
        "\n",
        "  It first tries to import modules and check for subclasses of `Tool` using\n",
        "  `issubclass`. If importing fails for any reason, it falls back to a\n",
        "  heuristic based on AST parsing to identify tool classes.\n",
        "\n",
        "  Args:\n",
        "    domain: The domain to search for tools in (e.g., 'retail', 'airline').\n",
        "\n",
        "  Returns:\n",
        "    A list of tool classes found for the given domain.\n",
        "  \"\"\"\n",
        "  tools_dir = PKG_ROOT / 'envs' / domain / 'tools'\n",
        "  if not tools_dir.exists():\n",
        "    return []\n",
        "  results = []\n",
        "  for pyfile in sorted(tools_dir.rglob('*.py')):\n",
        "    if pyfile.name == '__init__.py':\n",
        "      continue\n",
        "    modname = module_name_from_path(pyfile)\n",
        "    # Try import-based check first\n",
        "    try:\n",
        "      mod = importlib.import_module(modname)\n",
        "      for _, cls in inspect.getmembers(mod, inspect.isclass):\n",
        "        if getattr(cls, '__module__', None) != mod.__name__:\n",
        "          continue\n",
        "        if cls is Tool:\n",
        "          continue\n",
        "        try:\n",
        "          if issubclass(cls, Tool):\n",
        "            results.append(cls)\n",
        "        except Exception:\n",
        "          pass\n",
        "    except Exception:\n",
        "      # Import failed â†’ AST heuristic\n",
        "      results.extend(ast_inherits_tool(pyfile))\n",
        "  # Dedup preserve order\n",
        "  seen, deduped = set(), []\n",
        "  for q in results:\n",
        "    if q not in seen:\n",
        "      seen.add(q)\n",
        "      deduped.append(q)\n",
        "  return deduped\n",
        "\n",
        "tool_classes_by_domain = {d: collect_for_domain(d) for d in DOMAINS}\n",
        "tool_definitions_by_domain = {}\n",
        "for domain, tool_classes in tool_classes_by_domain.items():\n",
        "  tool_definitions_by_domain[domain] = []\n",
        "  for tool_class in tool_classes:\n",
        "    tool_info = tool_class.get_info()\n",
        "    if tool_info.get('type') != 'function' or not tool_info.get('function'):\n",
        "      continue\n",
        "    tool_definitions_by_domain[domain].append(tool_info['function'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQteYUPIM-HQ"
      },
      "source": [
        "# Classes and Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZVk-I-Kl1uL"
      },
      "outputs": [],
      "source": [
        "#@title Core Logic for Tau-Bench Evaluation\n",
        "# This section defines the main components for running tau-bench evaluations and\n",
        "# integrating them with GEPA:\n",
        "#   - A custom runner for tau-bench experiments.\n",
        "#   - Data structures for trajectories and outputs.\n",
        "#   - A GEPA adapter that bridges GEPA's optimization process with tau-bench.\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from datetime import datetime\n",
        "import os\n",
        "import json\n",
        "import multiprocessing\n",
        "import random\n",
        "from retry import retry\n",
        "import traceback\n",
        "from typing import List\n",
        "\n",
        "import tau_bench_agent as tau_bench_agent_lib\n",
        "from tau_bench.envs import get_env\n",
        "from tau_bench.run import display_metrics\n",
        "from tau_bench.types import EnvRunResult, RunConfig\n",
        "from litellm import provider_list\n",
        "from tau_bench.envs.user import UserStrategy\n",
        "\n",
        "\n",
        "def custom_run(\n",
        "    config: RunConfig,\n",
        "    print_results: bool = False,\n",
        "    custom_system_instruction: str = None) -> List[EnvRunResult]:\n",
        "  \"\"\"Runs a set of tau-bench tasks with a given agent configuration.\n",
        "\n",
        "  This is a customized version of the standard tau-bench run function, adapted\n",
        "  for this experiment's needs. It handles environment setup, agent creation,\n",
        "  task execution in parallel, and result aggregation.\n",
        "\n",
        "  Args:\n",
        "    config: A RunConfig object specifying the environment, models, and other\n",
        "      parameters for the run.\n",
        "    print_results: If True, prints the result of each task as it completes.\n",
        "    custom_system_instruction: An optional system instruction to use for the\n",
        "      agent, overriding the default.\n",
        "\n",
        "  Returns:\n",
        "    A list of EnvRunResult objects, one for each completed task.\n",
        "  \"\"\"\n",
        "  if config.env not in ['retail', 'airline']:\n",
        "    raise ValueError('Only retail and airline envs are supported')\n",
        "  if config.model_provider not in provider_list:\n",
        "    raise ValueError('Invalid model provider')\n",
        "  if config.user_model_provider not in provider_list:\n",
        "    raise ValueError('Invalid user model provider')\n",
        "  if config.agent_strategy not in [\n",
        "      'tool-calling', 'act', 'react', 'few-shot'\n",
        "  ]:\n",
        "    raise ValueError('Invalid agent strategy')\n",
        "  if config.task_split not in ['train', 'test', 'dev']:\n",
        "    raise ValueError('Invalid task split')\n",
        "  if config.user_strategy not in [item.value for item in UserStrategy]:\n",
        "    raise ValueError('Invalid user strategy')\n",
        "\n",
        "  random.seed(config.seed)\n",
        "  time_str = datetime.now().strftime('%m%d%H%M%S')\n",
        "  model_name = config.model.split('/')[-1]\n",
        "  ckpt_filename = (\n",
        "      f'{config.agent_strategy}-{model_name}-{config.temperature}_range_'\n",
        "      f'{config.start_index}-{config.end_index}_user-{config.user_model}-'\n",
        "      f'{config.user_strategy}_{time_str}.json'\n",
        "  )\n",
        "  ckpt_path = os.path.join(config.log_dir, ckpt_filename)\n",
        "  if not os.path.exists(config.log_dir):\n",
        "    os.makedirs(config.log_dir)\n",
        "\n",
        "  print(f'Loading user with strategy: {config.user_strategy}')\n",
        "  env = get_env(\n",
        "    config.env,\n",
        "    user_strategy=config.user_strategy,\n",
        "    user_model=config.user_model,\n",
        "    user_provider=config.user_model_provider,\n",
        "    task_split=config.task_split,\n",
        "  )\n",
        "  if custom_system_instruction:\n",
        "    env.wiki = custom_system_instruction\n",
        "  agent = tau_bench_agent_lib.adk_agent_factory(\n",
        "    tools_info=env.tools_info,\n",
        "    wiki=env.wiki,\n",
        "    config=config,\n",
        "  )\n",
        "  if config.end_index == -1:\n",
        "    end_index = len(env.tasks)\n",
        "  else:\n",
        "    end_index = min(config.end_index, len(env.tasks))\n",
        "  results: List[EnvRunResult] = []\n",
        "  lock = multiprocessing.Lock()\n",
        "  if config.task_ids and len(config.task_ids) > 0:\n",
        "    print(f'Running tasks {config.task_ids} (checkpoint path: {ckpt_path})')\n",
        "  else:\n",
        "    print(\n",
        "        f'Running tasks {config.start_index} to {end_index} '\n",
        "        f'(checkpoint path: {ckpt_path})'\n",
        "    )\n",
        "  for i in range(config.num_trials):\n",
        "    if config.task_ids and len(config.task_ids) > 0:\n",
        "      idxs = config.task_ids\n",
        "    else:\n",
        "      idxs = list(range(config.start_index, end_index))\n",
        "    if config.shuffle:\n",
        "      random.shuffle(idxs)\n",
        "\n",
        "    @retry(tries=3, delay=10, backoff=2)\n",
        "    def _run_with_retry(idx: int) -> EnvRunResult:\n",
        "      isolated_env = get_env(\n",
        "          config.env,\n",
        "          user_strategy=config.user_strategy,\n",
        "          user_model=config.user_model,\n",
        "          task_split=config.task_split,\n",
        "          user_provider=config.user_model_provider,\n",
        "          task_index=idx,\n",
        "      )\n",
        "      if print_results:\n",
        "        print(f'Running task {idx}')\n",
        "      res = agent.solve(\n",
        "          env=isolated_env,\n",
        "          task_index=idx,\n",
        "      )\n",
        "      return EnvRunResult(\n",
        "          task_id=idx,\n",
        "          reward=res.reward,\n",
        "          info=res.info,\n",
        "          traj=res.messages,\n",
        "          trial=i,\n",
        "      )\n",
        "\n",
        "    def _run(idx: int) -> EnvRunResult:\n",
        "      try:\n",
        "        result = _run_with_retry(idx)\n",
        "      except Exception as e:\n",
        "        logging.warning('Inference error: %s', str(e))\n",
        "        result = EnvRunResult(\n",
        "            task_id=idx,\n",
        "            reward=0.0,\n",
        "            info={'error': str(e), 'traceback': traceback.format_exc()},\n",
        "            traj=[],\n",
        "            trial=i,\n",
        "        )\n",
        "\n",
        "      if print_results:\n",
        "        print(\n",
        "            'âœ…' if result.reward == 1 else 'âŒ',\n",
        "            f'task_id={idx}',\n",
        "            # result.info,\n",
        "        )\n",
        "        print('-----')\n",
        "      with lock:\n",
        "        data = []\n",
        "        if os.path.exists(ckpt_path):\n",
        "          with open(ckpt_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        with open(ckpt_path, 'w') as f:\n",
        "          json.dump(data + [result.model_dump()], f, indent=2)\n",
        "      return result\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=config.max_concurrency) as executor:\n",
        "      res = list(executor.map(_run, idxs))\n",
        "      results.extend(res)\n",
        "\n",
        "  display_metrics(results)\n",
        "\n",
        "  with open(ckpt_path, 'w') as f:\n",
        "    json.dump([result.model_dump() for result in results], f, indent=2)\n",
        "    print(f'\\nðŸ“„ Results saved to {ckpt_path}\\n')\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZkwAFkINKG_"
      },
      "outputs": [],
      "source": [
        "#@title GEPA Adapter for Tau-bench\n",
        "# The TauBenchAdapter is the core component that allows GEPA to optimize prompts\n",
        "# for tau-bench. It implements the GEPAAdapter interface, defining how to:\n",
        "#   1. `evaluate`: Run a candidate prompt on a batch of tau-bench tasks and\n",
        "#      return scores.\n",
        "#   2. `make_reflective_dataset`: Convert evaluation results into a dataset that\n",
        "#      can be used for reflection and prompt improvement.\n",
        "\n",
        "from typing import List, Any, TypedDict\n",
        "import json\n",
        "import gepa\n",
        "from gepa.core.adapter import EvaluationBatch, GEPAAdapter\n",
        "\n",
        "\n",
        "class TauBenchDataInst(TypedDict):\n",
        "  env: str\n",
        "  task_id: int\n",
        "\n",
        "\n",
        "class TauBenchTrajectory(TypedDict):\n",
        "    result_traj: list[dict]\n",
        "\n",
        "\n",
        "class TauBenchRolloutOutput(TypedDict):\n",
        "    env: str\n",
        "    task_id: int\n",
        "    reward: float\n",
        "    task_info: dict\n",
        "\n",
        "\n",
        "def refine_tau_bench_trajectory(traj: list[dict[str, Any]]) -> None:\n",
        "  \"\"\"Removes unnecessary info from the trajectory, in place.\"\"\"\n",
        "  for content in traj:\n",
        "    for part in content[\"parts\"]:\n",
        "      # Drop all fields that are not populated.\n",
        "      to_drop = []\n",
        "      for key in part:\n",
        "        if not part[key]:\n",
        "          to_drop.append(key)\n",
        "      for key in to_drop:\n",
        "        del part[key]\n",
        "\n",
        "      # For function calls / responses only keep function names, input arguments\n",
        "      # and outputs.\n",
        "      if fc := part.get(\"function_call\"):\n",
        "        part[\"function_call\"] = dict(name=fc[\"name\"], args=fc[\"args\"])\n",
        "      if fr := part.get(\"function_response\"):\n",
        "        part[\"function_response\"] = dict(name=fr[\"name\"], args=fr[\"response\"])\n",
        "\n",
        "\n",
        "class TauBenchAdapter(GEPAAdapter[\n",
        "    TauBenchDataInst,\n",
        "    TauBenchTrajectory,\n",
        "    TauBenchRolloutOutput,\n",
        "]):\n",
        "  \"\"\"A GEPA adapter for evaluating agent performance on tau-bench benchmark.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    agent_model='gemini-2.5-flash',\n",
        "    agent_model_provider='vertex_ai',\n",
        "    user_model='gemini-2.5-pro',\n",
        "    user_model_provider='vertex_ai',\n",
        "    agent_strategy='tool-calling',\n",
        "    user_strategy='llm',\n",
        "    system_instruction_name='system_instruction',\n",
        "    tools_description: list[dict[str, Any]] | None = None,\n",
        "    max_concurrency=4,\n",
        "  ):\n",
        "    \"\"\"Initializes the TauBenchAdapter.\n",
        "\n",
        "    Args:\n",
        "      agent_model: The model to use for the agent.\n",
        "      agent_model_provider: The provider for the agent model.\n",
        "      user_model: The model to use for simulating the user.\n",
        "      user_model_provider: The provider for the user model.\n",
        "      agent_strategy: The agent strategy to use (e.g., 'tool-calling').\n",
        "      user_strategy: The user simulation strategy (e.g., 'llm').\n",
        "      system_instruction_name: The key in the candidate dictionary that holds\n",
        "        the system instruction.\n",
        "      tools_description: Describes each of the availble tools. This is used as context\n",
        "        for the prompt proposer.\n",
        "      max_concurrency: The maximum number of tasks to run in parallel.\n",
        "    \"\"\"\n",
        "    self._agent_model = agent_model\n",
        "    self._agent_model_provider = agent_model_provider\n",
        "    self._user_model = user_model\n",
        "    self._user_model_provider = user_model_provider\n",
        "    self._agent_strategy = agent_strategy\n",
        "    self._user_strategy = user_strategy\n",
        "    self._max_concurrency = max_concurrency\n",
        "    self._system_instruction_name = system_instruction_name\n",
        "    self._tools_description = tools_description\n",
        "\n",
        "  def evaluate(\n",
        "      self,\n",
        "      batch: list[TauBenchDataInst],\n",
        "      candidate: dict[str, str],\n",
        "      capture_traces: bool = False,\n",
        "  ) -> EvaluationBatch[TauBenchTrajectory, TauBenchRolloutOutput]:\n",
        "    \"\"\"Evaluates a candidate prompt on a batch of tau-bench tasks.\n",
        "\n",
        "    This method is called by GEPA during the optimization loop. It takes a\n",
        "    candidate prompt, runs it against the specified tasks from tau-bench, and\n",
        "    returns the results.\n",
        "\n",
        "    Args:\n",
        "      batch: A list of task instances to evaluate on. Each instance specifies\n",
        "        the environment and task ID.\n",
        "      candidate: A dictionary containing the components to be evaluated,\n",
        "        including the system instruction.\n",
        "      capture_traces: (Not used in this adapter) Whether to capture detailed\n",
        "        traces.\n",
        "\n",
        "    Returns:\n",
        "      An EvaluationBatch object containing scores, outputs, and trajectories for\n",
        "      each task in the batch.\n",
        "    \"\"\"\n",
        "    del capture_traces  # Not used.\n",
        "    env = batch[0]['env']\n",
        "    task_ids = [inst['task_id'] for inst in batch]\n",
        "    tau_bench_run_config = RunConfig(\n",
        "        env=env,\n",
        "        model=self._agent_model,\n",
        "        model_provider=self._agent_model_provider,\n",
        "        user_model=self._user_model,\n",
        "        user_model_provider=self._user_model_provider,\n",
        "        agent_strategy=self._agent_strategy,\n",
        "        user_strategy=self._user_strategy,\n",
        "        max_concurrency=self._max_concurrency,\n",
        "        task_ids=task_ids\n",
        "    )\n",
        "    tau_bench_results = custom_run(\n",
        "        tau_bench_run_config,\n",
        "        custom_system_instruction=candidate.get(self._system_instruction_name),\n",
        "    )\n",
        "\n",
        "    outputs = []\n",
        "    trajectories = []\n",
        "    scores = []\n",
        "    for res in tau_bench_results:\n",
        "      outputs.append(\n",
        "          TauBenchRolloutOutput(\n",
        "              env=env,\n",
        "              task_id=res.task_id,\n",
        "              reward=res.reward,\n",
        "              task_info=res.info))\n",
        "      result_traj = res.traj\n",
        "      refine_tau_bench_trajectory(result_traj)\n",
        "      trajectories.append(TauBenchTrajectory(result_traj=result_traj))\n",
        "      scores.append(res.reward)\n",
        "\n",
        "    return EvaluationBatch(\n",
        "        scores=scores, outputs=outputs, trajectories=trajectories)\n",
        "\n",
        "  def make_reflective_dataset(\n",
        "      self,\n",
        "      candidate: dict[str, str],\n",
        "      eval_batch: EvaluationBatch[TauBenchTrajectory, TauBenchRolloutOutput],\n",
        "      components_to_update: list[str]\n",
        "  ) -> dict[str, list[dict[str, Any]]]:\n",
        "    \"\"\"Creates a dataset for reflection based on evaluation results.\n",
        "\n",
        "    This method transforms the trajectories and scores from an evaluation run\n",
        "    into a structured format that a reflection model can use to generate\n",
        "    suggestions for improving the prompt.\n",
        "\n",
        "    Args:\n",
        "      candidate: The candidate that was evaluated.\n",
        "      eval_batch: The results of the evaluation.\n",
        "      components_to_update: A list of component names that the reflection\n",
        "        should focus on improving.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary where keys are component names and values are lists of\n",
        "      data instances for reflection.\n",
        "    \"\"\"\n",
        "    system_instruction = candidate[self._system_instruction_name]\n",
        "\n",
        "    tool_definitions = json.dumps(\n",
        "        self._tools_description,\n",
        "        indent=2,\n",
        "        default=str,\n",
        "    )\n",
        "\n",
        "    inputs = '\\n\\n'.join([\n",
        "        f'# System Instruction\\n{system_instruction}',\n",
        "        f'# Tool Definitions\\n{tool_definitions}',\n",
        "    ])\n",
        "    ret_d: dict[str, list[dict[str, Any]]] = {}\n",
        "    for comp in components_to_update:\n",
        "      items: list[dict[str, Any]] = []\n",
        "      trace_instances = list(zip(\n",
        "          eval_batch.trajectories,\n",
        "          eval_batch.scores,\n",
        "          eval_batch.outputs,\n",
        "          strict=True,\n",
        "      ))\n",
        "      for trace_instance in trace_instances:\n",
        "        traj, score, _ = trace_instance\n",
        "        if score > 0:\n",
        "          feedback = f'The agent successfully resolved all customer issues'\n",
        "        else:\n",
        "          feedback = (\n",
        "              f'The agent failed to resolve all customer issues correctly'\n",
        "          )\n",
        "        d = {\n",
        "            'Inputs': inputs,\n",
        "            'Generated Outputs': json.dumps(traj, indent=2, default=str),\n",
        "            'Feedback': feedback\n",
        "        }\n",
        "        items.append(d)\n",
        "      if items:\n",
        "        ret_d[comp] = items\n",
        "    assert ret_d, (\n",
        "        'empty reflective dataset for components '\n",
        "        f'{[comp for comp in components_to_update]}'\n",
        "    )\n",
        "    return ret_d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yQOx6WoNLGn"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE8_dE_DMqyk"
      },
      "outputs": [],
      "source": [
        "#@title Experiment Configuration and Execution\n",
        "# This section sets up and runs the GEPA optimization experiment.\n",
        "# Here we define all the parameters for the tau-bench environment, the GEPA\n",
        "# optimization loop, and the models to be used.\n",
        "\n",
        "#@markdown Tau-bench config\n",
        "tau_bench_env = 'retail' #@param ['retail', 'airline']\n",
        "agent_model = 'gemini-2.5-flash' #@param ['gemini-2.5-flash', 'gemini-2.0-flash']\n",
        "agent_model_provider = 'vertex_ai' #@param ['vertex_ai', 'google']\n",
        "user_model = 'gemini-2.5-flash' #@param ['gemini-2.5-flash', 'gemini-2.5-pro']\n",
        "user_model_provider = 'vertex_ai' #@param ['vertex_ai', 'google']\n",
        "max_concurrency = 8 #@param {type:\"integer\"}\n",
        "num_eval_trials = 4 #@param {type: 'integer'}\n",
        "#@markdown GEPA config\n",
        "training_set_size = 20 #@param {type:\"integer\"}\n",
        "eval_set_size = 20 #@param {type:\"integer\"}\n",
        "rnd_seed = 42 #@param {type:\"integer\"}\n",
        "max_metric_calls = 500 #@param {type:\"integer\"}\n",
        "reflection_model = 'gemini-2.5-pro' #@param ['gemini-2.5-flash', 'gemini-2.5-pro']\n",
        "reflection_minibatch_size = 3 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Dataset and Candidate Setup\n",
        "random.seed(rnd_seed)\n",
        "domain_to_size = {'retail': 115, 'airline': 50}\n",
        "all_tasks = set(range(domain_to_size[tau_bench_env]))\n",
        "training_task_ids = random.sample(list(all_tasks), training_set_size)\n",
        "eval_task_ids = random.sample(\n",
        "    list(all_tasks - set(training_task_ids)),\n",
        "    eval_set_size,\n",
        ")\n",
        "test_task_ids = list(all_tasks - set(training_task_ids) - set(eval_task_ids))\n",
        "\n",
        "training_set = [\n",
        "    TauBenchDataInst(env=tau_bench_env, task_id=task_id)\n",
        "    for task_id in training_task_ids\n",
        "]\n",
        "eval_set = [\n",
        "    TauBenchDataInst(env=tau_bench_env, task_id=task_id)\n",
        "    for task_id in eval_task_ids\n",
        "]\n",
        "\n",
        "system_instruction_name = 'system_instruction'\n",
        "\n",
        "SEED_SYSTEM_INSTRUCTION = (\n",
        "    'you are a customer support agent helping customers resolve their '\n",
        "    'issues by using the right tools'\n",
        ")\n",
        "\n",
        "seed_candidate = {\n",
        "    system_instruction_name: SEED_SYSTEM_INSTRUCTION,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ctYtM8HpMM8"
      },
      "outputs": [],
      "source": [
        "#@title Run GEPA Optimization\n",
        "# With the configuration and adapter in place, this section creates the adapter\n",
        "# instance and calls `gepa.optimize()` to start the Automatic Prompt\n",
        "# Optimization (APO) process.\n",
        "import litellm\n",
        "\n",
        "tau_bench_adapter = TauBenchAdapter(\n",
        "    agent_model=agent_model,\n",
        "    agent_model_provider=agent_model_provider,\n",
        "    user_model=user_model,\n",
        "    user_model_provider=user_model_provider,\n",
        "    agent_strategy='tool-calling',\n",
        "    user_strategy='llm',\n",
        "    system_instruction_name=system_instruction_name,\n",
        "    tools_description=tool_definitions_by_domain[tau_bench_env],\n",
        "    max_concurrency=max_concurrency,\n",
        ")\n",
        "\n",
        "gepa_results = gepa.optimize(\n",
        "    seed_candidate=seed_candidate,\n",
        "    trainset=training_set,\n",
        "    valset=eval_set,\n",
        "    task_lm=None, # this must be None when a custom adapter is used\n",
        "    adapter=tau_bench_adapter,\n",
        "    max_metric_calls=max_metric_calls,\n",
        "    reflection_lm = (\n",
        "        lambda prompt: litellm.completion_with_retries(\n",
        "            model=f'vertex_ai/{reflection_model}',\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            num_retries=4, initial_delay=1, max_delay=1,\n",
        "        ).choices[0].message.content\n",
        "    ),\n",
        "    reflection_minibatch_size=reflection_minibatch_size,\n",
        ")\n",
        "list(enumerate(gepa_results.val_aggregate_scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3awKFTnbBLW"
      },
      "outputs": [],
      "source": [
        "#@title Evaluate All Candidates\n",
        "\n",
        "\n",
        "# This is the prompt from https://arxiv.org/pdf/2406.12045\n",
        "DEFAULT_SYSTEM_INSTRUCTION = '''# Retail agent policy\n",
        "\n",
        "As a retail agent, you can help users cancel or modify pending orders, return or exchange delivered orders, modify their default user address, or provide information about their own profile, orders, and related products.\n",
        "\n",
        "- At the beginning of the conversation, you have to authenticate the user identity by locating their user id via email, or via name + zip code. This has to be done even when the user already provides the user id.\n",
        "\n",
        "- Once the user has been authenticated, you can provide the user with information about order, product, profile information, e.g. help the user look up order id.\n",
        "\n",
        "- You can only help one user per conversation (but you can handle multiple requests from the same user), and must deny any requests for tasks related to any other user.\n",
        "\n",
        "- Before taking consequential actions that update the database (cancel, modify, return, exchange), you have to list the action detail and obtain explicit user confirmation (yes) to proceed.\n",
        "\n",
        "- You should not make up any information or knowledge or procedures not provided from the user or the tools, or give subjective recommendations or comments.\n",
        "\n",
        "- You should at most make one tool call at a time, and if you take a tool call, you should not respond to the user at the same time. If you respond to the user, you should not make a tool call.\n",
        "\n",
        "- You should transfer the user to a human agent if and only if the request cannot be handled within the scope of your actions.\n",
        "\n",
        "## Domain basic\n",
        "\n",
        "- All times in the database are EST and 24 hour based. For example \"02:30:00\" means 2:30 AM EST.\n",
        "\n",
        "- Each user has a profile of its email, default address, user id, and payment methods. Each payment method is either a gift card, a paypal account, or a credit card.\n",
        "\n",
        "- Our retail store has 50 types of products. For each type of product, there are variant items of different options. For example, for a 't shirt' product, there could be an item with option 'color blue size M', and another item with option 'color red size L'.\n",
        "\n",
        "- Each product has an unique product id, and each item has an unique item id. They have no relations and should not be confused.\n",
        "\n",
        "- Each order can be in status 'pending', 'processed', 'delivered', or 'cancelled'. Generally, you can only take action on pending or delivered orders.\n",
        "\n",
        "- Exchange or modify order tools can only be called once. Be sure that all items to be changed are collected into a list before making the tool call!!!\n",
        "\n",
        "## Cancel pending order\n",
        "\n",
        "- An order can only be cancelled if its status is 'pending', and you should check its status before taking the action.\n",
        "\n",
        "- The user needs to confirm the order id and the reason (either 'no longer needed' or 'ordered by mistake') for cancellation.\n",
        "\n",
        "- After user confirmation, the order status will be changed to 'cancelled', and the total will be refunded via the original payment method immediately if it is gift card, otherwise in 5 to 7 business days.\n",
        "\n",
        "## Modify pending order\n",
        "\n",
        "- An order can only be modified if its status is 'pending', and you should check its status before taking the action.\n",
        "\n",
        "- For a pending order, you can take actions to modify its shipping address, payment method, or product item options, but nothing else.\n",
        "\n",
        "### Modify payment\n",
        "\n",
        "- The user can only choose a single payment method different from the original payment method.\n",
        "\n",
        "- If the user wants the modify the payment method to gift card, it must have enough balance to cover the total amount.\n",
        "\n",
        "- After user confirmation, the order status will be kept 'pending'. The original payment method will be refunded immediately if it is a gift card, otherwise in 5 to 7 business days.\n",
        "\n",
        "### Modify items\n",
        "\n",
        "- This action can only be called once, and will change the order status to 'pending (items modifed)', and the agent will not be able to modify or cancel the order anymore. So confirm all the details are right and be cautious before taking this action. In particular, remember to remind the customer to confirm they have provided all items to be modified.\n",
        "\n",
        "- For a pending order, each item can be modified to an available new item of the same product but of different product option. There cannot be any change of product types, e.g. modify shirt to shoe.\n",
        "\n",
        "- The user must provide a payment method to pay or receive refund of the price difference. If the user provides a gift card, it must have enough balance to cover the price difference.\n",
        "\n",
        "## Return delivered order\n",
        "\n",
        "- An order can only be returned if its status is 'delivered', and you should check its status before taking the action.\n",
        "\n",
        "- The user needs to confirm the order id, the list of items to be returned, and a payment method to receive the refund.\n",
        "\n",
        "- The refund must either go to the original payment method, or an existing gift card.\n",
        "\n",
        "- After user confirmation, the order status will be changed to 'return requested', and the user will receive an email regarding how to return items.\n",
        "\n",
        "## Exchange delivered order\n",
        "\n",
        "- An order can only be exchanged if its status is 'delivered', and you should check its status before taking the action. In particular, remember to remind the customer to confirm they have provided all items to be exchanged.\n",
        "\n",
        "- For a delivered order, each item can be exchanged to an available new item of the same product but of different product option. There cannot be any change of product types, e.g. modify shirt to shoe.\n",
        "\n",
        "- The user must provide a payment method to pay or receive refund of the price difference. If the user provides a gift card, it must have enough balance to cover the price difference.\n",
        "\n",
        "- After user confirmation, the order status will be changed to 'exchange requested', and the user will receive an email regarding how to return items. There is no need to place a new order.\n",
        "'''\n",
        "\n",
        "\n",
        "all_system_instructions = [\n",
        "    DEFAULT_SYSTEM_INSTRUCTION,\n",
        "    SEED_SYSTEM_INSTRUCTION,\n",
        "    gepa_results.best_candidate['system_instruction'],\n",
        "]\n",
        "\n",
        "\n",
        "system_instruction_to_eval_results = {}\n",
        "for system_instruction in all_system_instructions:\n",
        "  tau_bench_run_config = RunConfig(\n",
        "      env=tau_bench_env,\n",
        "      model=agent_model,\n",
        "      model_provider=agent_model_provider,\n",
        "      user_model=user_model,\n",
        "      user_model_provider=user_model_provider,\n",
        "      agent_strategy='tool-calling',\n",
        "      user_strategy='llm',\n",
        "      max_concurrency=max_concurrency,\n",
        "      num_trials=num_eval_trials,\n",
        "      task_ids=test_task_ids,\n",
        "  )\n",
        "  tau_bench_results = custom_run(\n",
        "      tau_bench_run_config,\n",
        "      custom_system_instruction=system_instruction,\n",
        "  )\n",
        "  total = len(tau_bench_results)\n",
        "  numerator = sum(1 for res in tau_bench_results if res.reward == 1)\n",
        "  print(\n",
        "      f'average reward (total={total}): {numerator/total if total > 0 else 0}'\n",
        "  )\n",
        "  system_instruction_to_eval_results[system_instruction] = tau_bench_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Q5hMuERuO6"
      },
      "outputs": [],
      "source": [
        "print(gepa_results.best_candidate['system_instruction'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbG7aBXLRuO6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/language/tunelab/tunekit/colab:colab_notebook",
        "kind": "private"
      },
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
